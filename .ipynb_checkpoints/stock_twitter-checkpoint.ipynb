{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import time\n",
    "import tweepy\n",
    "from datetime import datetime\n",
    " \n",
    "# Import and Initialize Sentiment Analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Twitter API Keys\n",
    "from tw_config import (consumer_key,\n",
    "                       consumer_secret,\n",
    "                       access_token,\n",
    "                       access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Security</th>\n",
       "      <th>GICS Sector</th>\n",
       "      <th>GICS Sub Industry</th>\n",
       "      <th>Location</th>\n",
       "      <th>Date Added</th>\n",
       "      <th>CIK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M Company</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "      <td>St. Paul, Minnesota</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott Laboratories</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>3/31/1964</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie Inc.</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Pharmaceuticals</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>12/31/2012</td>\n",
       "      <td>1551152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABMD</td>\n",
       "      <td>ABIOMED Inc</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>Danvers, Massachusetts</td>\n",
       "      <td>5/31/2018</td>\n",
       "      <td>815094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture plc</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>IT Consulting &amp; Other Services</td>\n",
       "      <td>Dublin, Ireland</td>\n",
       "      <td>7/6/2011</td>\n",
       "      <td>1467373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticker             Security             GICS Sector  \\\n",
       "0    MMM           3M Company             Industrials   \n",
       "1    ABT  Abbott Laboratories             Health Care   \n",
       "2   ABBV          AbbVie Inc.             Health Care   \n",
       "3   ABMD          ABIOMED Inc             Health Care   \n",
       "4    ACN        Accenture plc  Information Technology   \n",
       "\n",
       "                GICS Sub Industry                 Location  Date Added  \\\n",
       "0        Industrial Conglomerates      St. Paul, Minnesota         NaN   \n",
       "1           Health Care Equipment  North Chicago, Illinois   3/31/1964   \n",
       "2                 Pharmaceuticals  North Chicago, Illinois  12/31/2012   \n",
       "3           Health Care Equipment   Danvers, Massachusetts   5/31/2018   \n",
       "4  IT Consulting & Other Services          Dublin, Ireland    7/6/2011   \n",
       "\n",
       "       CIK  \n",
       "0    66740  \n",
       "1     1800  \n",
       "2  1551152  \n",
       "3   815094  \n",
       "4  1467373  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load S&P500 Company List\n",
    "df_sp500 = pd.read_csv('Resources/SP500_Company_List.csv')\n",
    "\n",
    "df_sp500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n",
      "'NoneType' object has no attribute 'group'\n"
     ]
    }
   ],
   "source": [
    "# Find Twitter handles for S&P500 Companies - RUN IF \"SP500_twtr_ids.csv\" DOES NOT ALREADY EXIST. \n",
    "twtr_ids = []\n",
    "\n",
    "# Search for Twitter handles by Company Name\n",
    "for index, row in df_sp500.iterrows(): \n",
    "    try:\n",
    "        user_id = ''\n",
    "        screen_name = ''\n",
    "        name = ''\n",
    "        \n",
    "        time.sleep(1)                                                  # Keep API Calls under limit\n",
    "        public_users = api.search_users(row.Security)\n",
    "        re_name = re.search('[\\w]+', row.Security)\n",
    "        loc = re.search('[\\w]+$', row.Location)\n",
    "        if len(public_users) > 0:\n",
    "            for user in public_users:\n",
    "                if (re_name.group(0).lower() in user['name'].lower()):\n",
    "                    user_id = user['id']\n",
    "                    screen_name = user['screen_name']\n",
    "                    name = user['name']\n",
    "                    break\n",
    "                \n",
    "                elif (row.Security.lower() in user['name'].lower()):\n",
    "                    user_id = user['id']\n",
    "                    screen_name = user['screen_name']\n",
    "                    name = user['name']\n",
    "                \n",
    "        twtr_ids.append([row.Ticker, row.Security, loc.group(0).lower(), user_id, screen_name, name])\n",
    "        #print(f'Company Match: [{row.Ticker},{row.Security},{loc.group(0).lower()},{user_id},{screen_name},{name}]')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# Search for Twitter handles with different company name search\n",
    "for i in range(len(twtr_ids)):\n",
    "    try:\n",
    "        if twtr_ids[i][3] == '':                                     # Check if user_id is blank\n",
    "            time.sleep(1)                                                # Keep API Calls under limit\n",
    "            re_name = re.search('[\\w]+ [\\w]+', twtr_ids[i][1])\n",
    "            if re_name != None:\n",
    "                public_users = api.search_users(re_name.group(0).lower())\n",
    "            \n",
    "                if len(public_users) > 0:\n",
    "                    for user in public_users:\n",
    "                        if (re_name.group(0).lower() in user['name'].lower()):\n",
    "                            user_id = user['id']\n",
    "                            screen_name = user['screen_name']\n",
    "                            name = user['name']\n",
    "                            twtr_ids[i] = [twtr_ids[i][0],twtr_ids[i][1],twtr_ids[i][2],user_id,screen_name,name]\n",
    "                            #print(f'2nd Company Search: {twtr_ids[i]}')\n",
    "                            break\n",
    "                           \n",
    "                        elif (twtr_ids[i][1].lower() in user['name'].lower()):\n",
    "                            user_id = user['id']\n",
    "                            screen_name = user['screen_name']\n",
    "                            name = user['name']\n",
    "                            twtr_ids[i] = [twtr_ids[i][0],twtr_ids[i][1],twtr_ids[i][2],user_id,screen_name,name]\n",
    "                            #print(f'2nd Company Search: {twtr_ids[i]}')\n",
    "                            \n",
    "    except Exception as e:\n",
    "        print(e)        \n",
    "        \n",
    "# Search for Twitter handles by ticker symbol\n",
    "for i in range(len(twtr_ids)):\n",
    "    try:\n",
    "        if twtr_ids[i][3] == '':                                     # Check if user_id is blank\n",
    "            time.sleep(1)                                                # Keep API Calls under limit\n",
    "            public_users = api.search_users(f'${twtr_ids[i][0]}')\n",
    "            \n",
    "            if len(public_users) > 0:\n",
    "                for user in public_users:\n",
    "                    if re.search('[\\w]+', twtr_ids[i][1]) != None:\n",
    "                        if re.search('[\\w]+', twtr_ids[i][1]).group(0).lower() in user['name'].lower():\n",
    "                            user_id = user['id']\n",
    "                            screen_name = user['screen_name']\n",
    "                            name = user['name']\n",
    "                            twtr_ids[i] = [twtr_ids[i][0],twtr_ids[i][1],twtr_ids[i][2],user_id,screen_name,name]\n",
    "                            #print(f'Ticker Match: {twtr_ids[i]}')\n",
    "                            break\n",
    "                        \n",
    "                    elif re.search('[\\w]+ [\\w]+', twtr_ids[i][1]) != None:\n",
    "                        if re.search('[\\w]+ [\\w]+', twtr_ids[i][1]).group(0).lower() in user['name'].lower():\n",
    "                            user_id = user['id']\n",
    "                            screen_name = user['screen_name']\n",
    "                            name = user['name']\n",
    "                            twtr_ids[i] = [twtr_ids[i][0],twtr_ids[i][1],twtr_ids[i][2],user_id,screen_name,name]\n",
    "                            #print(f'Ticker Match: {twtr_ids[i]}')\n",
    "                            break\n",
    "                        \n",
    "                    elif (twtr_ids[i][0] == user['name']) and (user['followers_count'] > 10000):\n",
    "                        user_id = user['id']\n",
    "                        screen_name = user['screen_name']\n",
    "                        name = user['name']\n",
    "                        twtr_ids[i] = [twtr_ids[i][0],twtr_ids[i][1],twtr_ids[i][2],user_id,screen_name,name]\n",
    "                        #print(f'Ticker Match: {twtr_ids[i]}')\n",
    "                      \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "df_twtr_ids = pd.DataFrame(twtr_ids, columns=['ticker','security','location','user_id','screen_name','twtr_name'])\n",
    "df_twtr_ids = df_twtr_ids.drop(columns=['location'])\n",
    "df_twtr_ids.to_csv('Resources/SP500_twtr_ids.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'message': 'Rate limit exceeded', 'code': 88}]\n",
      "[{'message': 'Rate limit exceeded', 'code': 88}]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-803a1f6d2805>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# Perform search of most recent 100 tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mpublic_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_term\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get Twitter Sentiments for S&P500 Companies\n",
    "twtr_sentiments = []\n",
    "\n",
    "for index, row in df_sp500.iterrows(): \n",
    "    try:\n",
    "        # Create search name to use in Twitter search\n",
    "        re_name = re.search('[\\w]+ [\\w]+', row.Security)\n",
    "        \n",
    "        # Verify that search name is not null \n",
    "        if re_name == None: \n",
    "            re_name = re.search('[\\w]+', row.Security)\n",
    "            \n",
    "        # Create Twitter search term\n",
    "        target_term = f'{re_name.group(0)} ${row.Ticker}'\n",
    "        \n",
    "        # Perform search of most recent 100 tweets\n",
    "        time.sleep(5)\n",
    "        public_tweets = api.search(target_term, count=100)\n",
    "        \n",
    "        for tweet in public_tweets['statuses']:\n",
    "            twt_date = datetime.strptime(tweet[\"created_at\"], \"%a %b %d %H:%M:%S %z %Y\")\n",
    "            twt_fdate = twt_date.strftime('%Y%m%d')\n",
    "            twt_yr = twt_date.strftime('%Y')\n",
    "            twt_md = twt_date.strftime('%m%d')\n",
    "            \n",
    "            twt_text = tweet['text']\n",
    "            vdr_results = analyzer.polarity_scores(twt_text)\n",
    "            compound = vdr_results['compound']\n",
    "            neg = vdr_results['neg']\n",
    "            neu = vdr_results['neu']\n",
    "            pos = vdr_results['pos']\n",
    "            \n",
    "            # Create row for Twitter sentiment list\n",
    "            twtr_sentiments.append([row.Ticker, row.Security, twt_fdate, twt_yr, twt_md, compound, neg, neu, pos, twt_text])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "#Create dataframe for Twitter sentiments\n",
    "data_columns = ('ticker','company','date','year','monthday','compound_score','neg_score','neu_score','pos_score','tweet')\n",
    "\n",
    "df_twtr_sentiments = pd.DataFrame(twtr_sentiments, columns=data_columns)\n",
    "\n",
    "qtr_bins = [0,331,630,930,1231]\n",
    "qtr_labels = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "\n",
    "twt_qtr = pd.cut(pd.to_numeric(df_twtr_sentiments.monthday, errors='coerce'), qtr_bins, labels=qtr_labels)\n",
    "\n",
    "quarter = []\n",
    "\n",
    "for x in range(len(twt_qtr)):\n",
    "    quarter.append(str(df_twtr_sentiments.year[x]) + str(twt_qtr[x]))\n",
    "\n",
    "df_twtr_sentiments['quarter'] = quarter\n",
    "\n",
    "df_twtr_sentiments = df_twtr_sentiments.drop(columns=['monthday'])\n",
    "\n",
    "df_twtr_sentiments.to_csv('Resources/SP500_sentiments.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
